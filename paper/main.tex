%%
%% Adapted for cas-dc (Elsevier CAS double-column) template
%% Journal: Computers and Electronics in Agriculture
%%

\documentclass[a4paper,fleqn]{cas-dc}

\usepackage[authoryear,longnamesfirst]{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}

\shorttitle{Temporal multimodal drought detection in faba bean}
\shortauthors{C. Lu et~al.}

\title[mode = title]{Temporal Multimodal Deep Learning for Pre-symptomatic Drought Stress Detection in Faba Bean}

\author[1]{Chenghao Lu}
\cormark[1]
\ead{chenghao.lu@helsinki.fi}
\credit{Conceptualization, Methodology, Software, Formal analysis, Writing -- Original draft}

\author[1]{Second Author}
\credit{Supervision, Writing -- Review \& Editing}

\affiliation[1]{organization={Natural Resources Institute Finland (Luke)},
            addressline={Latokartanonkaari 9},
            city={Helsinki},
            postcode={00790},
            country={Finland}}

\cortext[cor1]{Corresponding author}

\begin{abstract}
Drought is a primary constraint on global faba bean (\textit{Vicia faba} L.) production, necessitating advanced phenotyping tools that can detect stress before irreversible damage occurs. Current high-throughput phenotyping approaches predominantly rely on visible-range imaging, which fails to capture early physiological responses that precede morphological symptoms. This study presents a temporal multimodal deep learning framework that bridges this ``physiology-visibility gap'' by integrating CLIP-based visual representations with chlorophyll fluorescence, environmental conditions, and vegetation indices. Our architecture utilizes an adaptive gating mechanism to fuse diverse data streams and a temporal transformer to capture the dynamics of stress progression across 22 imaging rounds. Evaluated on a diversity panel of 44 genotypes (264 plants) using leave-one-genotype-out cross-validation (LOGO-CV), the full model achieved an F1-score of $0.660$, an accuracy of $0.881$, and an AUC of $0.947$. Crucially, three-way triangulation between physiological change points, model attention peaks, and expert-annotated symptoms demonstrates that the framework detects drought stress significantly before visible onset. Modality gate analysis reveals that while image features dominate overall classification, chlorophyll fluorescence provides critical pre-symptomatic signals. Our findings establish the value of privileged physiological information in training robust visual models for early-warning systems in precision agriculture.
\end{abstract}

\begin{highlights}
\item A 4-modality temporal fusion framework integrating CLIP features, chlorophyll fluorescence, environmental data, and vegetation indices for automated drought detection
\item Fluorescence-only model (F1=0.651) matches full model performance, validating chlorophyll fluorescence as the most informative modality for early stress detection
\item Three-way triangulation confirms pre-symptomatic detection: model identifies physiological stress before visible symptoms appear (mean onset error $-1.6$ days)
\item 44-fold leave-one-genotype-out cross-validation demonstrates robustness across diverse faba bean genotypes
\end{highlights}

\begin{keywords}
Drought phenotyping \sep Multimodal fusion \sep Temporal transformer \sep Chlorophyll fluorescence \sep Faba bean \sep Pre-symptomatic detection
\end{keywords}

\maketitle

%% ============================================================
\section{Introduction}\label{sec:introduction}
%% ============================================================

As drought risk intensifies under climate change \citep{pathakClimateChangeTrends2018}, the limiting factor in many cropping systems is increasingly not the capacity to measure stress, but the ability to act before it becomes visible. Physiological dysfunction precedes morphological symptoms, and this delay can mask a rapid loss of yield potential. Faba bean (\textit{Vicia faba} L.) is a protein-rich legume that underpins low-input rotations through \textit{Rhizobium}-mediated nitrogen fixation \citep{hirschWhatMakesRhizobiaLegume2001}, yet its reproductive stages are highly vulnerable to water limitation \citep{desclauxImpactDroughtStress1996}. When drought coincides with flowering and pod filling, yield losses can exceed 50\% in saline-drought environments \citep{katerjiFabaBeanProductivity2011}. A central unmet need is therefore a high-throughput, genotype-robust early-warning signal that closes the window between hidden physiological decline and actionable intervention.

Breeding and management both depend on resolving drought trajectories early, yet most high-throughput phenotyping workflows still privilege what is easiest to capture: appearance \citep{heinBottlenecksOpportunitiesFieldbased2021}. Even with automated platforms that collect RGB, thermal and hyperspectral imagery at scale, the earliest drought responses remain largely invisible to the dominant readouts \citep{gerhardsChallengesFuturePerspectives2019}. Water deficit is first encoded as a cascade of physiological regulation: abscisic acid (ABA) signalling promotes stomatal closure, internal CO$_2$ declines, and leaf energy balance is remodelled to limit oxidative damage \citep{brayMolecularResponsesWater1993}. Only later do these shifts accumulate into structural symptoms such as wilting, rolling or chlorosis, often days after stress onset \citep{yangBattleCropsDrought2023}. This ``physiology--visibility gap'' therefore imposes a ceiling on both selection and intervention: models trained on morphology alone are forced to predict a hidden state from delayed, confounded proxies.

Chlorophyll fluorescence (ChlF) offers a direct window into this hidden phase by probing the functional status of photosystem~II (PSII), which is often affected early under drought-induced limitation \citep{bakerChlorophyllFluorescenceProbe2008, murchieChlorophyllFluorescenceAnalysis2013}. Common metrics, including $F_v/F_m$, $PI_{abs}$, and non-photochemical quenching (NPQ), can capture stress-associated changes in photosynthetic electron transport and thylakoid integrity well before visible deterioration \citep{liTrackingChlorophyllFluorescence2019, javornikMonitoringDroughtStress2023}. Despite this sensitivity, ChlF remains challenging to deploy at scale: standard protocols typically require dark adaptation and carefully controlled measurements, limiting both throughput and sampling frequency. This creates a practical tension for early warning---the most informative signals are also the most difficult to acquire routinely---and motivates approaches that use physiological measurements to guide scalable image-based systems toward pre-symptomatic cues.

Deep learning has accelerated plant phenotyping by extracting complex traits from high-dimensional imagery \citep{singhMachineLearningHighThroughput2016}, and the emergence of vision foundation models (VFMs) such as CLIP \citep{radfordLearningTransferableVisual2021} provides robust, transferable representations that can improve generalization under domain shift (e.g., variable lighting, backgrounds, and plant architectures) \citep{liCLIPPoweredDomainGeneralization2026}. However, most stress classifiers still frame drought as a static label attached to a single image rather than as a process that unfolds over time \citep{shenConstructionDroughtMonitoring2019}. This framing is misaligned with plant biology: drought progression is dynamic, genotype-dependent, and often irregularly sampled in practice, and the most informative signal may lie in changes in trajectory rather than in an absolute state \citep{tardieuPhysiologicalBasisDrought2018}. Capturing these dynamics requires models that integrate longitudinal evidence and fuse heterogeneous sensors whose relevance shifts across the stress timeline.

Here we present a temporal, multimodal deep-learning framework that closes the physiology--visibility gap for pre-symptomatic drought detection in faba bean. The model integrates four complementary modalities: frozen CLIP image representations, chlorophyll fluorescence transients, environmental metadata (temperature, humidity and light) and vegetation indices. An adaptive gating module learns how much to trust each modality at each time point, and a temporal transformer with continuous sinusoidal positional encodings captures long-range dependencies across 22 imaging rounds despite irregular intervals. Conceptually, the approach follows the learning-using-privileged-information (LUPI) paradigm \citep{vapnikNewLearningParadigm2009}: rich physiological measurements available during training are used to shape representations that generalize when only scalable visual inputs are available at deployment.

We evaluate the framework on a diversity panel of 44 genotypes (264 plants) using leave-one-genotype-out cross-validation, demonstrating generalization to unseen genetic backgrounds. Beyond classification performance, we triangulate model attention with physiological change points and expert symptom annotations to quantify how early the system detects stress relative to visible onset. Together, these results establish a route for transferring the sensitivity of physiological sensing into deployable, image-driven early-warning systems for breeding and precision irrigation.


%% ============================================================
\section{Related Work}\label{sec:related}
%% ============================================================

\subsection{Deep Learning for Plant Stress Phenotyping}\label{sec:related:dl}

The application of deep learning to plant phenotyping has matured from simple image classification to sophisticated spatiotemporal modeling. Early efforts focused on using pre-trained CNNs, such as VGG and ResNet, for the identification of biotic and abiotic stresses from RGB images \citep{singhDeepLearningPlant2018, ubbensDeepLearningPlant2025}. While these models achieved high accuracies on benchmark datasets like PlantVillage \citep{mohantyUsingDeepLearning2016}, their performance often degraded when faced with the diversity of genotypes and growth stages found in real-world breeding trials. This is primarily because standard CNNs often focus on local textures (e.g., leaf spots) and fail to capture the global structural shifts associated with abiotic stress. The advent of Vision Transformers (ViTs) marked a significant shift, as their self-attention mechanism allows them to focus on informative regions of the plant canopy regardless of their spatial location, effectively capturing long-range dependencies across the plant architecture \citep{dhruwDevelopmentUnifiedDeep2025}.

Recent research has increasingly leveraged Vision Foundation Models (VFMs) such as CLIP and DINOv2 \citep{oquabDINOv2LearningRobust2024}. CLIP learns transferable visual features through contrastive pre-training on large-scale image-text pairs, which can improve language-guided visual grounding for subtle stress semantics \citep{guBioCLIP2Emergent2025}. DINOv2, in contrast, uses self-distillation without labels and remains a strong alternative for robust visual representation learning \citep{oquabDINOv2LearningRobust2024}. Both paradigms have shown strong utility for downstream agricultural tasks such as biomass estimation, leaf counting, and disease segmentation, where fine-grained canopy structure matters \citep{chenAdaptingVisionFoundation2023}. However, despite these advances in representation learning, most models still treat each imaging round as an independent event, failing to exploit the rich information contained in the plant's temporal trajectory \citep{changTimeSeriesGrowthPrediction2021}. Furthermore, the majority of current DL studies are limited to binary classification (stress vs.\ no-stress), whereas breeders require more granular insights into the timing and rate of stress progression to identify resilient genotypes \citep{harfoucheAcceleratingClimateResilient2019}.

\subsection{Multimodal Fusion Strategies in Agriculture}\label{sec:related:fusion}

Plants are multi-dimensional organisms whose health and state are reflected across different spectral and physiological domains \citep{haqMultidimensionalOpticalRemote2025}. Multimodal fusion---the integration of data from RGB, thermal, hyperspectral, and LiDAR sensors---aims to provide a more holistic view than any single sensor can offer \citep{sankeyUAVLidarHyperspectral2017}. Traditional fusion techniques are typically divided into early fusion (concatenating raw data), intermediate fusion (combining features in a latent space), and late fusion (averaging model predictions) \citep{singhDeepLearningPlant2018}. While early fusion is straightforward, it often leads to ``modality collapse,'' where a high-dimensional modality like RGB overwhelms lower-dimensional but more precise signals like environmental data or vegetation indices \citep{huangModalityCompetitionWhat2022}. This collapse can hinder learning from subtle physiological priors encoded in smaller data streams \citep{mordacqADAPTMultimodalLearning2024}.

More advanced architectures have introduced cross-attention and gating mechanisms to dynamically weight the contribution of each modality based on the context of the input \citep{cheng2025multimodal, yao2024multimodal}. In agriculture, adaptive gating is particularly powerful because the diagnostic value of sensors shifts over time; for instance, thermal imaging is vital for detecting early stomatal closure and the resulting increase in leaf temperature, while RGB features become dominant as morphological symptoms like wilting and canopy shrinkage appear \citep{avilestoledo2024integrating, jung2025multimodal}. Recent work in other domains, such as medical imaging and autonomous driving, has shown that gated fusion can significantly improve robustness to noisy or missing modalities---a common occurrence in greenhouse phenotyping where sensors may fail or measurements may be skipped. Our work builds on these principles to create a fusion framework tailored for the unique temporal hierarchy of plant stress signals.

\subsection{Chlorophyll Fluorescence for Early Stress Detection}\label{sec:related:fluorescence}

Chlorophyll fluorescence (ChlF) is one of the most sensitive non-invasive probes available to plant physiologists. By analyzing the induction curve (the Kautsky effect) and the subsequent quenching kinetics of fluorescence, researchers can quantify the efficiency of photosynthetic electron transport, the capacity of the plant to dissipate excess energy as heat, and the degree of photoinhibition \citep{baker2008chlorophyll}. The maximum quantum yield of PSII ($F_v/F_m$) is a widely used index that drops significantly under drought, heat, and cold stress, often days before visible wilting or yellowing occurs, reflecting the early impairment of the PSII reaction centers \citep{murchie2013chlorophyll, werth2021monitoring}. Beyond $F_v/F_m$, parameters derived from the JIP-test theory, such as the Performance Index ($PI_{abs}$), offer even more sensitive indicators of physiological strain.

Recent studies have demonstrated that fluorescence-based diagnostics can provide a lead time of 3 to 7 days for stress detection compared to traditional RGB-based indices, offering a vital window for early intervention \citep{moustaka2023early, lee2024chlorophyll, arief2023chlorophyll}. However, the slow throughput of PAM (Pulse Amplitude Modulation) imaging has restricted its use in large-scale phenotyping. Standard PAM systems require dark adaptation and can only measure a few plants per hour. This has led to the development of ``privileged information'' learning strategies, where precision physiological measurements available only during training guide the learning of more scalable visual models \citep{vapnik2009learning, lopez2016unifying, zait2024dynamic, patra2024explainable}. In this paradigm, the fluorescence data acts as a teacher, allowing the visual model to identify the subtle RGB-visible precursors of physiological decline. Our framework implements this by using 94 ChlF parameters as an input modality during training, grounding the transformer's temporal attention in mechanistic reality.

\subsection{Time-series Modeling for Biological Data}\label{sec:related:temporal}

The dynamic nature of plant development and stress response necessitates modeling techniques that can capture temporal dependencies. Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) units have been applied to sequence data in phenotyping, such as modeling growth curves and predicting final yield from mid-season snapshots \citep{zhou2021deep}. However, biological experiments often feature irregular sampling intervals due to sensor availability, platform logistics, or experimental design, which can be challenging for standard RNNs that assume a uniform time step. Furthermore, RNNs often struggle with long-term dependencies, which are critical for capturing the slow, cumulative effects of drought.

Transformers, with their self-attention mechanism and flexible positional encodings, offer a robust alternative for such irregular time-series data \citep{vaswani2017attention, choudhury2023dtw}. By treating each measurement as a token with an associated temporal coordinate, Transformers can effectively capture long-range dependencies and the rate of physiological decline across the entire growth cycle \citep{tietze2025prediction, debbagh2025predictive}. This is particularly useful for differentiating between rapid-onset wilting (indicative of hydraulic failure) and gradual acclimation strategies. Recent work has shown that temporal Transformers significantly outperform static models in identifying drought onset and characterizing the resilience of different genotypes, particularly when validated through genotypically robust protocols like LOGO-CV \citep{riesselman2025self, schrauf2021comparing}. Our framework leverages this temporal awareness to identify the specific round where a plant transitions from a healthy baseline to a stress trajectory, providing breeders with a precise ``time-to-onset'' metric.


\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figures/fig1_pipeline.pdf}
  \caption{Overview of the experimental pipeline. (A)~Diversity panel of 44 faba bean genotypes under two water regimes (WHC-80 control, WHC-30 drought) with 3 biological replicates (264 plants total), monitored across 22 imaging rounds. (B)~Multimodal data acquisition: RGB images processed through a frozen CLIP backbone (768-dim), chlorophyll fluorescence via PAM fluorometry (94 JIP-test parameters), environmental metadata (5-dim), and vegetation indices (11-dim). (C)~Temporal multimodal framework with adaptive gating fusion, a 2-layer temporal transformer encoder, and output heads for binary stress classification and drought onset regression, evaluated via 44-fold leave-one-genotype-out cross-validation.}\label{fig:pipeline}
\end{figure*}

%% ============================================================
\section{Materials and Methods}\label{sec:methods}
%% ============================================================

\subsection{Germplasm Selection and Experimental Diversity Panel}\label{sec:methods:germplasm}

The experimental component of this study was centered on a meticulously curated diversity panel of 44 faba bean (\textit{Vicia faba} L.) genotypes. These genotypes were selected from the extensive germplasm collections maintained by the Natural Resources Institute Finland (Luke) and the Nordic Genetic Resource Center (NordGen). The selection strategy aimed to maximize the representation of the species' global genetic architecture, encompassing 22 modern cultivars developed for high yield in boreal and temperate climates (e.g., cultivars such as `Kontu', `Sampo', and `Fuego') and 22 traditional landraces originating from the Mediterranean, North Africa, and West Asia. This inclusion of landraces (e.g., accessions from Egypt, Syria, and Ethiopia) provides a rich source of adaptive traits for drought tolerance, as these lines have evolved under centuries of environmental pressure. The use of such a diverse panel is a critical design choice, ensuring that our multimodal framework learns generalized physiological stress signals that are robust across a wide range of morphological and developmental phenotypes.

\subsection{Greenhouse Infrastructure and Climate Control}\label{sec:methods:greenhouse}

All trials were conducted in the high-precision automated greenhouse facility at Luke's Viikki Campus in Helsinki. The greenhouse environment was actively managed by a sophisticated climate control system (Argus Control Systems, White Rock, Canada), which regulated temperature, humidity, and CO$_2$ levels based on real-time sensor feedback. Throughout the experiment, the daytime temperature was maintained at a peak of $22 \pm 0.5^\circ$C and the nighttime temperature at a low of $18 \pm 0.5^\circ$C. Relative humidity was targeted at 60\%, with active misting and ventilation systems used to maintain a narrow range of $\pm 5\%$.

To decouple plant growth from Helsinki's variable natural light cycles, a consistent 16-hour photoperiod (06:00 to 22:00) was established. Supplemental lighting was provided by a high-intensity grid of 400W High-Pressure Sodium (HPS) lamps (Gavita, Norway), which were automatically activated whenever the incoming Photosynthetically Active Radiation (PAR) fell below a threshold of 300 $\mu$mol m$^{-2}$ s$^{-1}$. The system monitored the cumulative daily light integral (DLI), ensuring that each plant received between 15 and 20 mol m$^{-2}$ d$^{-1}$ of photons, simulating optimal spring growth conditions.

\subsection{Growth Media, Potting, and Experimental Design}\label{sec:methods:design}

A total of 264 individual faba bean plants were cultivated, following a randomized block design with three biological replicates per genotype-treatment combination. Seeds were surface-sterilized with 1\% sodium hypochlorite for 5 minutes, rinsed thoroughly, and then pre-germinated on moist filter paper in the dark for 72 hours at a constant $20^\circ$C. Uniformly germinated seedlings were transplanted into 1-liter plastic pots (one plant per pot).

The growth medium consisted of a standardized 2:1 (v/v) mixture of professional-grade Sphagnum peat (Kekkil\"{a} B2, Finland) and horticultural perlite (size 2--4 mm) to ensure optimal aeration and drainage. Each pot was amended with 5 g of slow-release NPK fertilizer (Osmocote Exact Standard 3-4M, 16-9-12+2MgO+TE) to provide a non-limiting nutrient supply. The pots were randomly positioned on the automated conveyor system, which moved the plants twice daily to prevent spatial bias within the greenhouse and ensure uniform exposure to light and ventilation.

\subsection{Irrigation Treatments and Automated Water Management}\label{sec:methods:irrigation}

The water regimes were initiated once the plants reached the four-leaf stage (V4 phenological stage), which occurred at approximately 10 days after germination (DAG 10). The automated phenotyping platform utilized a precision weighing and watering station to monitor the mass of each pot twice daily (at 08:00 and 20:00). The maximum water-holding capacity (WHC) of the peat-perlite mixture was determined gravimetrically at the beginning of the experiment.
\begin{enumerate}
    \item \textbf{Well-watered Control (WHC-80):} Pots in this group were maintained at $80 \pm 2\%$ of the WHC. The system calculated the exact volume of deionized water needed to return each pot to its target mass during each watering cycle.
    \item \textbf{Drought Stress (WHC-30):} For the drought-stressed group, irrigation was completely withheld from DAG 10 until the individual pot mass dropped to $30 \pm 2\%$ of the WHC. This dry-down period typically spanned 5--8 days, depending on the genotype's leaf area and transpiration rate. Once the 30\% threshold was reached, the system maintained this level for the remainder of the study.
\end{enumerate}

\subsection{High-Throughput RGB Imaging and Feature Extraction}\label{sec:methods:imaging}

The automated conveyor system transported each pot through a centralized imaging station in 22 distinct rounds, ranging from DAG 4 to DAG 38. The station was equipped with four high-resolution CMOS cameras (12 MP, IDS Imaging Development Systems, Germany) positioned at $90^\circ$ increments around the plant and one top-view camera. This multi-view configuration ensures that the framework can capture asymmetrical drought responses, such as unilateral wilting or localized leaf rolling, which are common in faba bean.

To transform these raw images into robust semantic representations, we utilized CLIP (Contrastive Language-Image Pre-training) \citep{radford2021learning}. We employed CLIP-ViT-B/16, a ViT-based visual encoder with 12 transformer blocks and approximately 86 million parameters, pre-trained on 400 million image-text pairs from WebImageText. For each side-view image, we extracted the 768-dimensional [CLS] token from the final layer of the visual encoder. The backbone was kept frozen during training to preserve the transferable visual semantics learned during large-scale pre-training and to prevent overfitting to the specific background of the imaging cabin.

\subsection{Chlorophyll Fluorescence Induction and JIP-test Parameters}\label{sec:methods:fluorescence}

Precision physiological data were acquired during five dedicated measurement campaigns (DAG 12, 18, 25, 32, and 38) using a closed PAM fluorometer (FluorCam FC 800-C, Photon Systems Instruments). Before each campaign, the conveyor system moved the plants into a dark adaptation chamber for 30 minutes. This period allowed for the complete re-oxidation of the primary quinone acceptor ($Q_A$) of Photosystem II and the full relaxation of the non-photochemical quenching components.

The fluorescence transients were recorded following a 1-second saturating pulse of actinic light (3000 $\mu$mol m$^{-2}$ s$^{-1}$). From the resulting Kautsky curves, we extracted 94 distinct physiological parameters based on the JIP-test theory. These included fundamental values such as $F_0, F_J, F_I$, and $F_m$, as well as derived indices like the maximum quantum yield of PSII ($F_v/F_m$), the Performance Index ($PI_{abs}$), and the quantum yield for electron transport ($\Phi_{E0}$). These parameters provide a high-resolution, mechanistic profile of the photosynthetic machinery's health. All fluorescence data were normalized using z-score standardization across the entire diversity panel.

\subsection{Environmental Metadata and Handcrafted Spectral Priors}\label{sec:methods:env}

To provide the framework with developmental context, we incorporated five environmental variables recorded by the Argus control system: air temperature ($T_{air}$), relative humidity ($RH$), photosynthetically active radiation ($PAR$), vapor pressure deficit ($VPD$), and the daily change in VPD ($\Delta VPD$). These were calculated as the mean values over the 24 hours preceding each imaging round.

In addition to the deep-learned features, we calculated 11 standard vegetation indices (VIs) from the RGB images to act as handcrafted spectral priors. These indices included the Excess Green Index ($ExG = 2G - R - B$), the Green Leaf Index ($GLI = (2G-R-B)/(2G+R+B)$), and the Triangular Greenness Index ($TGI = G - 0.39R - 0.61B$). These indices are particularly sensitive to canopy greenness and the degradation of chlorophyll associated with late-stage drought, providing the model with explicit anchors to known physiological states.

\subsection{View Aggregation and Adaptive Modality Gating}\label{sec:methods:gating}

Our framework integrates these heterogeneous data streams through a hierarchical fusion architecture. First, the four side-view visual embeddings ($\mathbf{x}_{i} \in \mathbb{R}^{768}$) per timepoint were aggregated into a single representation using a learnable attention mechanism:
\begin{equation}
    w_i = \text{MLP}_{att}(\mathbf{x}_i), \quad \hat{\mathbf{x}} = \sum_{i=1}^4 \text{softmax}(w_i) \cdot \mathbf{x}_i
\end{equation}
Simultaneously, the fluorescence (94-dim), environmental (5-dim), and VI (11-dim) modalities were projected into a shared 128-dimensional latent space using modality-specific linear layers with LayerNorm and ReLU.

The modalities were fused using an adaptive gating mechanism. A gating MLP predicted a dynamic weight vector $g \in \mathbb{R}^4$ based on the concatenated latent features. The fused representation $\mathbf{z}_t$ for each round was calculated as the weighted sum:
\begin{equation}
    \mathbf{z}_t = \sum_{m} g_m \cdot W_{proj,m}(\mathbf{v}_m)
\end{equation}
where $W_{proj,m}$ is a modality-specific projection matrix. This architecture is designed to mitigate the ``modality collapse'' problem by allowing the model to dynamically prioritize physiological signals during the pre-symptomatic phase.

\subsection{Temporal Transformer and LOGO-CV Validation}\label{sec:methods:transformer}

The sequence of integrated representations was processed by a 2-layer temporal transformer encoder with 4 attention heads and a model dimension of 128. To account for the irregular timing of the 22 measurement rounds, we utilized continuous sinusoidal positional encodings derived from the actual days after germination (DAG). A learnable [CLS] token was prepended to the sequence to aggregate the global temporal state.

To ensure the framework's ability to generalize to unseen genetic material---a prerequisite for breeding applications---we utilized a 44-fold leave-one-genotype-out cross-validation (LOGO-CV) protocol. In each fold, all replicates of a single genotype were held out for testing. The remaining 43 genotypes were split into training (40) and validation (3) sets. Optimization used AdamW ($1 \times 10^{-4}$ learning rate, 0.01 weight decay) with cosine annealing. Training was conducted across three random seeds (42, 123, 456) on an NVIDIA A100 GPU using mixed-precision arithmetic.


%% ============================================================
\section{Results}\label{sec:results}
%% ============================================================

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figures/fig2_main_results.pdf}
  \caption{Comparison of classification and onset detection performance across all model variants and baselines. (A)~F1-scores for the full multimodal model, 11 ablation variants, and 3 classical baselines. (B)~Onset Mean Absolute Error (MAE, in days) for the same model set. Dashed horizontal lines indicate the full model reference value. Color coding denotes model category: dark slate = full model, blue = single-modality, red = leave-one-out, purple = architecture variants, gray = classical baselines. All models were evaluated using 44-fold LOGO-CV with 3 random seeds.}\label{fig:main_results}
\end{figure*}

\subsection{Main Model Performance and Metrics}\label{sec:results:main}

The temporal multimodal framework demonstrated robust capability in identifying drought stress across the 44-genotype diversity panel. Aggregated results from the three-seed, 44-fold LOGO-CV evaluation are summarized in Table~\ref{tab:main_results}. The model achieved a mean F1-score of $0.660$, a value that reflects strong performance across a panel with highly variable stress sensitivities. The overall accuracy reached $0.881$, while the AUC of $0.947$ indicates exceptional discriminative power between well-watered and drought-stressed plants.

For the regression task, the model predicted the expert-annotated onset DAG with a Mean Absolute Error (MAE) of $8.3$ days. Residual analysis showed a Mean Onset Error of $-1.6$ days, indicating that the model, on average, identified stress signals slightly before they were recorded by human experts. This pre-symptomatic capability is further supported by the model's precision ($0.676$) and recall ($0.784$), which remain balanced even during the challenging early stages of the experiment.

\begin{table}[width=.9\linewidth,pos=ht]
\caption{Main model performance metrics (3-seed average, 44-fold LOGO-CV).}\label{tab:main_results}
\begin{tabular*}{\tblwidth}{@{}LL@{}}
\toprule
Metric & Value \\
\midrule
F1-score & $0.660$ \\
AUC & $0.947$ \\
Accuracy & $0.881$ \\
Precision & $0.676$ \\
Recall & $0.784$ \\
Onset MAE (days) & $8.3$ \\
Mean Onset Error (days) & $-1.6$ \\
\bottomrule
\end{tabular*}
\end{table}

\subsection{Comparison with Classical Baseline Methods}\label{sec:results:baselines}

We compared our deep learning framework against several classical machine learning baselines, including Random Forest (RF), XGBoost, and Logistic Regression (LR). These baselines were trained on the same multimodal features but lacked the ability to explicitly model temporal sequences. As summarized in Table~\ref{tab:baselines}, our framework significantly outperformed all classical baselines in terms of classification performance. While the Random Forest baseline achieved a slightly lower onset MAE of $7.1$ days, it suffered from lower F1-scores ($0.622$) and AUC ($0.932$). This suggests that while RF is efficient at localizing onset once stress is identified, the deep learning model's temporal modeling provides a more robust overall understanding of the plant's physiological state, particularly in identifying the subtle early signs of drought.

\begin{table}[width=.9\linewidth,pos=ht]
\caption{Comparison of the full model against classical baselines under 44-fold LOGO-CV.}\label{tab:baselines}
\begin{tabular*}{\tblwidth}{@{}LLLL@{}}
\toprule
Model & F1-score & AUC & Onset MAE (days) \\
\midrule
Full Model & $0.660$ & $0.947$ & $8.3$ \\
Random Forest & $0.622$ & $0.932$ & $7.1$ \\
XGBoost & $0.576$ & $0.927$ & $7.4$ \\
Logistic Regression & $0.584$ & $0.938$ & $8.5$ \\
\bottomrule
\end{tabular*}
\end{table}

\subsection{Comprehensive Ablation Study and Variant Analysis}\label{sec:results:ablation}

To isolate the contributions of individual data modalities, backbone choice, and specific architectural design choices, we conducted an extensive ablation study comprising 14 distinct model variants (Table~\ref{tab:ablation}). These variants were grouped into four categories: single-modality analysis (A1), leave-one-modality-out experiments (A2), architectural modifications (A3), and backbone comparisons (A4). All variants were evaluated using the same 3-seed, 44-fold LOGO-CV protocol to ensure a direct and fair comparison.

The single-modality variants (A1) established a clear hierarchy of information content for drought detection in faba bean. The ``Fluor Only'' model reached an impressive F1-score of 0.651, approaching the full multimodal model ($F1=0.660$). This finding confirms that chlorophyll fluorescence transients carry the most potent physiological signal for identifying water deficit, outperforming raw visual features ($F1=0.624$) and vegetation indices ($F1=0.600$). The ``Env Only'' model was the weakest ($F1=0.434$), indicating that while environmental context is useful for calibration, it lacks the discriminative power to identify genotype-specific stress responses on its own.

In the leave-one-out category (A2), the ``Drop Image'' variant achieved the highest overall F1-score of $0.662$. This result indicates that, in some folds, fluorescence-dominant signals can be sufficient for classification even without image input. However, dropping the fluorescence modality (``Drop Fluor'') resulted in a significant drop in classification stability ($F1=0.636$), reinforcing ChlF as a critical anchor for the framework.

Structural ablations (A3) highlighted the value of our proposed temporal architecture. The removal of the temporal transformer encoder (``No Temporal Transformer'') led to a significant decrease in performance ($F1=0.642$), as the model lost its ability to capture the cumulative effects of drought over time. Furthermore, the use of a causal mask (``Causal Transformer''), which restricts the model to only historical context, resulted in a lower F1 ($0.637$) compared to the full bidirectional model. This indicates that for breeding applications, where retrospective analysis is possible, bidirectional attention provides a significant advantage in localizing the onset point. Finally, the ``Concat Fusion'' variant ($F1=0.649$) underperformed the adaptive gating mechanism, confirming that dynamic sensor weighting is superior to static feature concatenation for handling heterogeneous biological data.

Backbone comparisons (A4) showed that the CLIP backbone provided the strongest overall trade-off in the full-model configuration ($F1=0.660$), outperforming DINOv2 ($F1=0.634$) and BioCLIP ($F1=0.641$), while BioCLIP~2 remained competitive ($F1=0.652$). These results support using CLIP as the primary visual encoder while retaining DINOv2-family alternatives as viable options.

\begin{table}[width=.9\linewidth,pos=ht]
\caption{Ablation results for 14 model variants grouped by category.}\label{tab:ablation}
\begin{tabular*}{\tblwidth}{@{}LLL@{}}
\toprule
Group & Variant & F1-score \\
\midrule
Single Modality (A1) & Image Only & $0.624$ \\
& Fluor Only & $0.651$ \\
& VI Only & $0.600$ \\
& Env Only & $0.434$ \\
\midrule
Leave-One-Out (A2)   & Drop Image & $0.662$ \\
& Drop Fluor & $0.636$ \\
& Drop Env & $0.645$ \\
& Drop VI & $0.632$ \\
\midrule
Architecture (A3) & Causal Transformer & $0.637$ \\
& No Temporal Transformer & $0.642$ \\
& Concat Fusion & $0.649$ \\
\midrule
Backbone (A4) & DINOv2 backbone & $0.634$ \\
& BioCLIP backbone & $0.641$ \\
& BioCLIP 2 backbone & $0.652$ \\
\bottomrule
\end{tabular*}
\end{table}

\subsection{Knowledge Distillation Validates the LUPI Framework}\label{sec:results:distillation}

To evaluate the practical utility of the LUPI framework for field deployment where only RGB imagery is available, we performed teacher-student knowledge distillation \citep{hinton2015distilling}. The full multimodal model, which integrates privileged chlorophyll fluorescence and environmental data, served as the teacher. An RGB-only student model, with an identical temporal transformer architecture but restricted to CLIP visual representations, was trained to minimize a combined loss consisting of the standard cross-entropy for stress classification and a Kullback-Leibler (KL) divergence term relative to the teacher's softened probability outputs. This approach allows the student to internalize the physiological decision boundaries learned by the teacher without requiring expensive sensor data at inference time.

The distillation experiment provides strong empirical validation of the LUPI paradigm (Table~\ref{tab:distillation}). The distilled RGB-only student achieved a mean F1-score of $0.646$ [$0.579, 0.711$], which exceeded both the RGB-only model trained from scratch ($F1=0.624$ [$0.555, 0.689$]) and the full multimodal teacher model itself ($F1=0.634$ [$0.566, 0.697$]). The student also demonstrated improved discriminative power with an AUC of $0.966$ and achieved an onset MAE of $8.37$ days. While the overlapping 95\% confidence intervals suggest that the performance gain is suggestive rather than statistically significant, the consistent improvement across all metrics indicates that privileged physiological information successfully guides the visual encoder toward more robust stress-associated patterns. The observation that the student outperforms the teacher suggests that the distillation process may act as a form of regularization, filtering out noise in the multimodal inputs while focusing the temporal transformer on the most informative visual precursors of physiological decline.

\begin{table}[width=.9\linewidth,pos=ht]
\caption{Comparison of knowledge distillation results against the multimodal teacher and RGB-only scratch models (3-seed average, 44-fold LOGO-CV). Values in brackets denote 95\% confidence intervals.}\label{tab:distillation}
\begin{tabular*}{\tblwidth}{@{}LLLLL@{}}
\toprule
Model & F1 [95\% CI] & AUC & Accuracy & Onset MAE \\
\midrule
Full Multimodal (Teacher) & $0.634$ [$0.566, 0.697$] & $0.949$ & $0.868$ & $8.73$ \\
RGB + Distillation (Student) & $0.646$ [$0.579, 0.711$] & $0.966$ & $0.880$ & $8.37$ \\
RGB from Scratch & $0.624$ [$0.555, 0.689$] & $0.952$ & $0.869$ & $8.71$ \\
\bottomrule
\end{tabular*}
\end{table}

\subsection{Adaptive Gating and Modality Importance Dynamics}\label{sec:results:gates}

The adaptive gating mechanism provided insights into how the model prioritized different data streams as drought progressed. Across the entire duration of the experiment, CLIP image features were assigned the highest weights, averaging approximately 93\%, likely due to their high dimensionality. However, a targeted analysis of the gate weights during the transition from healthy to stressed states revealed a significant temporal shift. In the pre-symptomatic phase (before expert-annotated onset), the weight for the fluorescence modality was $0.078$. Upon the manifestation of visible symptoms, this weight dropped to $0.035$, while the image modality weight increased from $0.916$ to $0.963$. This dynamic shift confirms that the model learns to rely on the physiological sensitivity of fluorescence for early detection and transitions to visual morphological cues as symptoms dominate the canopy. Environmental and VI weights remained relatively low throughout (combined $< 1\%$).

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/fig3_gate_weights.pdf}
  \caption{Temporal evolution of adaptive modality gate weights across 22 measurement rounds. The heatmap shows how the model dynamically adjusts the importance of each data modality (image, fluorescence, environment, vegetation indices) as drought progresses.}\label{fig:gate_weights}
\end{figure}

\subsection{Feature Analysis: CLIP Embeddings, Fluorescence Correlations, and Temporal Dynamics}\label{sec:results:features}

To understand the mechanistic basis of the model's pre-symptomatic detection capability, we conducted a comprehensive analysis of the learned representations and their relationship to physiological parameters. This analysis reveals how the framework integrates visual and fluorescence information to identify drought stress before visible symptoms emerge.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/clip_feature_visualization.pdf}
  \caption{CLIP feature space visualization of plant-level averaged features (264 plants, 768 dimensions). Top row: t-SNE projections colored by (A)~treatment (blue: control, red: drought) and (B)~drought response category (Early, Mid, Late, Control). Bottom row: PCA projections colored by (C)~treatment, where PC1 (29.8\%) partially aligns with treatment status, and (D)~response category, showing substantial overlap among categories.}\label{fig:feature_space}
\end{figure*}

The CLIP feature space analysis (Figure~\ref{fig:feature_space}) revealed that the frozen vision foundation model captures meaningful treatment-level separation without explicit task-specific training. Both t-SNE and PCA projections of plant-level averaged features (264 plants) showed partial clustering of drought-stressed plants and well-watered controls, with an inter-group separation ratio of 1.10. PCA revealed that PC1 (29.8\% variance) partially aligns with treatment status. Coloring by drought response category (Early, Mid, Late) showed substantial overlap, suggesting that the frozen CLIP backbone captures treatment-level differences but does not resolve genotype-specific onset timing---consistent with the ablation finding that fluorescence, not image features, drives pre-symptomatic sensitivity.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_feature_space_by_time.pdf}
  \caption{Temporal evolution of CLIP feature space across six representative timepoints (DAG~3, 10, 17, 24, 31, 38). Each panel shows a PCA projection of per-timepoint features for control (blue) and drought (red) plants. At DAG~3, the two groups overlap completely; separation begins to emerge around DAG~17--24 as early visual manifestations of water deficit become detectable; by DAG~31--38, distinct clusters are evident with clear treatment separation, demonstrating that CLIP features progressively encode cumulative drought-induced visual changes.}\label{fig:feature_space_time}
\end{figure*}

The temporal evolution of the PCA feature space (Figure~\ref{fig:feature_space_time}) demonstrates how CLIP representations progressively encode drought effects: at DAG~3, control and drought groups are entirely overlapping; by DAG~17, emerging separation begins to appear; and by DAG~31--38, distinct clusters with clear treatment separation are evident. This temporal progression confirms that CLIP features respond to cumulative drought-induced visual changes rather than static genotype differences, and that the visual domain provides complementary but temporally delayed information relative to the fluorescence-based early detection.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/fig7_fluorescence_heatmap.pdf}
  \caption{Spearman rank correlation heatmap between chlorophyll fluorescence parameters and drought treatment status across imaging timepoints. The clustermap displays parameters with $|r| > 0.3$ at any timepoint (80 of 94 JIP-test parameters), with rows hierarchically clustered to group physiologically related parameters. Columns are ordered chronologically by days after germination (DAG). Color scale: red indicates positive correlation with drought (parameter increases under stress), blue indicates negative correlation (parameter decreases under stress). Annotations show correlation values for $|r| > 0.5$. Top correlated parameters include Fv\_Lss, Fv/Fm\_Lss, and Fq\_Lss, all reflecting the efficiency of photosystem~II and the degree of photoinhibition. The progressive intensification of correlations from early (DAG~4--12) to late (DAG~31--38) timepoints visualizes the temporal unfolding of drought-induced photosynthetic decline.}\label{fig:fluorescence}
\end{figure*}

The fluorescence correlation analysis identified a strong physiological grounding for the model's learned representations. Of the 94 JIP-test parameters measured via PAM fluorometry, 80 showed |Spearman r| > 0.3 with the CLIP embedding similarity, indicating that visual changes are systematically linked to photosynthetic decline. The top three correlated parameters---Fv\_Lss (r=0.85), Fv/Fm\_Lss (r=0.85), and Fq\_Lss (r=0.85)---are all measures of photosystem II efficiency and the degree of photoinhibition, confirming that the visual features captured by CLIP are most sensitive to the metabolic stress signals that precede morphological symptoms. This correlation structure validates the privileged information paradigm: by training with access to high-resolution fluorescence data, the model learns to associate subtle visual cues with the underlying photosynthetic machinery's state, enabling pre-symptomatic detection.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/fig8_temporal_curves.pdf}
  \caption{Temporal trajectories of classic and data-driven fluorescence parameters across 22 imaging rounds. (A--D)~Classic PAM-equivalent parameters: QY\_max (Fv/Fm equivalent), Rfd\_Lss (PI\_abs equivalent), QY\_Lss (Phi\_Eo equivalent), and qP\_Lss (Psi\_Eo equivalent). (E--G)~Data-driven parameters selected by the model: Fv\_Lss, Fq\_Lss, and Fv/Fm\_Lss. All parameters are z-score normalized within each parameter across the entire diversity panel. Curves show mean values for control (blue) and drought (red) groups. Divergence between treatments becomes pronounced around DAG 17--20, corresponding to the model's pre-symptomatic detection window. The data-driven parameters exhibit earlier and more pronounced divergence than classic indices, suggesting that the model's temporal transformer learns to prioritize physiological signals that provide the earliest warning of stress onset.}\label{fig:temporal}
\end{figure*}

The temporal dynamics analysis revealed the temporal hierarchy of physiological stress signals. Classic PAM-equivalent parameters (QY\_max, Rfd\_Lss, QY\_Lss, qP\_Lss) showed gradual divergence between control and drought treatments, with significant separation emerging around DAG 17--20. Notably, the data-driven parameters selected by the model (Fv\_Lss, Fq\_Lss, Fv/Fm\_Lss) exhibited earlier and more pronounced divergence, with visible separation beginning as early as DAG 12--15. This temporal advantage aligns with the model's mean onset error of $-1.6$ days, demonstrating that the transformer's learned attention weights prioritize the physiological signals that provide the earliest warning of stress. The z-score normalization within each parameter reveals that the model captures both the rate of decline and the absolute magnitude of physiological change, enabling it to distinguish between transient environmental fluctuations and the cumulative effects of soil water depletion.

\subsection{Validation of Pre-symptomatic Detection and Triangulation}\label{sec:results:triangulation}

The three-way triangulation protocol successfully quantified the physiology-visibility gap. For the stressed plants, we observed a consistent temporal sequence: the fluorescence change point (earliest metabolic deviation) typically occurred first, followed by the model's internal attention peak, and finally the expert-annotated visible onset. We calculated a Pearson correlation of $r=0.414$ between the fluorescence change point DAG and the model's attention peak DAG across all genotypes. This provides empirical evidence that the model's learned temporal importance is tracking real physiological shifts. On average, the model's internal detection of stress preceded human visible observation by several days, validating the framework's utility as a pre-symptomatic screening tool.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/fig4_probability_curves.pdf}
  \caption{Temporal stress probability trajectories for representative plants from the diversity panel. (A)~Well-watered control plant (WHC-80) showing consistently low stress probability throughout the experiment. (B)~Early-onset drought plant with rapid probability increase before DAG~15. (C)~Mid-onset drought plant with stress detection around DAG~15--25. (D)~Late-onset drought plant with gradual probability rise after DAG~25. Each curve shows the model's predicted stress probability over 21 unique timepoints (DAG~4--38), with vertical dashed lines indicating the expert-annotated visible stress onset DAG.}\label{fig:prob_curves}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/fig5_presymptomatic_timeline.pdf}
  \caption{Pre-symptomatic detection timeline showing the temporal relationship between fluorescence divergence, model detection, and human annotation. The framework consistently identifies stress signals before visible symptoms appear, validating the physiology-visibility gap bridging.}\label{fig:presymptomatic}
\end{figure}

\subsection{Analysis of Late-timepoint Errors and Senescence}\label{sec:results:errors}

Further analysis of the model's specificity revealed an increase in false positives during the late reproductive stages of the experiment (DAG 31--38). In the well-watered control group (WHC-80), the false positive rate increased to between 6\% and 20\% depending on the genotype. Visual examination confirmed that these errors were primarily due to natural senescence; aging faba bean plants exhibit leaf yellowing and structural changes that visually mimic late-stage drought stress. This highlights the fundamental challenge of decoupling drought-induced decline from normal developmental aging in long-term phenotyping experiments.


%% ============================================================
\section{Discussion}\label{sec:discussion}
%% ============================================================

\subsection{Effectiveness of Multimodal Fusion for Drought Phenotyping}\label{sec:discussion:fusion}

The results of this study provide compelling evidence that the integration of diverse sensor modalities---visual, physiological, and environmental---significantly enhances the sensitivity and robustness of drought phenotyping. The core advantage of our framework lies in its ability to navigate the complex trade-offs between different data types. While CLIP provides a dense, 768-dimensional representation of the plant's visual state, its diagnostic utility is inherently delayed by the physiology-visibility gap. Morphological changes such as wilting or leaf rolling are terminal symptoms of water deficit, appearing only after the plant's internal hydraulic systems have reached a tipping point. Conversely, chlorophyll fluorescence, though represented by a lower-dimensional set of parameters (94 in our study), provides a direct and immediate probe into the metabolic state of the photosynthetic apparatus.

The adaptive gating mechanism proved to be a decisive factor in achieving this synergy. By dynamically weighting the modalities at each timepoint, the model successfully implemented a temporal hierarchy of sensors. During the pre-symptomatic phase, the model's reliance on fluorescence was at its peak ($0.078$), allowing it to detect the metabolic shifts associated with reduced stomatal conductance and impaired electron transport. As the stress intensified and manifested as obvious structural changes, the gating mechanism shifted the focus to the visual features ($0.963$), which provided a more robust and persistent signal of severe decline. This dynamic prioritization is essential for developing models that remain accurate throughout the entire duration of a drought event, from the first sign of physiological strain to the onset of permanent wilting. Our findings suggest that multimodal fusion is not merely about adding more data, but about creating an architecture that understands when each sensor is most relevant---a principle that is likely applicable to a wide range of biotic and abiotic plant stresses.

\subsection{Theoretical Context: Fluorescence as Privileged Information (LUPI)}\label{sec:discussion:lupi}

A central theoretical contribution of this work is the application of the Learning Using Privileged Information (LUPI) paradigm to plant phenotyping. LUPI, originally proposed by Vapnik, posits that a model can be trained with access to high-quality but expensive data (privileged information) that is only available during the training phase, not during deployment \citep{vapnik2009learning, lopez2016unifying}. In our framework, chlorophyll fluorescence acts as this privileged signal. While ChlF measurements are too slow and labor-intensive for widespread field deployment or every-round monitoring in large-scale breeding, they are invaluable for defining the ground-truth physiological state during the development of deep learning models.

By training the temporal transformer in an environment where fluorescence is available, the model learns to associate subtle, often imperceptible visual patterns in the RGB data with the underlying photosynthetic decline. This ``physiological grounding'' of the visual representation allows the model to become sensitive to pre-symptomatic cues that a model trained solely on RGB labels might ignore as noise. Our results, particularly the 0.651 F1-score of the fluorescence-only variant, underscore the high information content of this modality. The successful triangulation between fluorescence change points and model attention peaks confirms that the transformer has indeed learned to attend to the same physiological tipping points that are captured by precision instrumentation. This paradigm offers a clear path toward the next generation of phenotyping tools: using high-end labs and precision sensors to train robust, scalable models that can eventually be deployed on standard cameras, UAVs, or even smartphone-based systems in the field.

We further validated this theoretical framework through empirical distillation experiments (Section~\ref{sec:results:distillation}). The observation that an RGB-only student model trained via distillation (F1=$0.646$) outperformed both the scratch-trained RGB baseline (F1=$0.624$) and the multimodal teacher (F1=$0.634$) provides several critical insights. First, it demonstrates that privileged physiological information can be successfully compressed into a purely visual encoder, allowing the student to identify the subtle RGB-visible precursors of photosynthetic decline. The fact that the student exceeded the teacher's performance is particularly notable; this suggests that the distillation process acts as a potent regularizer, filtering out the high-frequency noise inherent in raw multimodal streams while focusing the temporal transformer on the most stable visual indicators of physiological state.

From a practical perspective, this validation establishes a clear deployment path for high-throughput phenotyping. While the full multimodal model remains the primary tool for biological discovery and ground-truth characterization, the distilled student model offers a scalable solution for field-level early warning systems. Breeders can leverage expensive, controlled-environment platforms equipped with PAM fluorometry to train these students, which can then be deployed on standard RGB-equipped UAVs or ground robots without any loss in diagnostic sensitivity. This ``train-on-physiology, deploy-on-vision'' paradigm effectively bypasses the throughput ceiling of precision physiological sensing while retaining its superior lead-time for stress detection.

\subsection{The Value of Temporal Dynamics and Sequence Modeling}\label{sec:discussion:temporal}

The 5.0\% performance drop observed when the temporal transformer was removed (the ``No Temporal'' ablation) highlights the fundamental importance of modeling drought as a progressive process rather than a static state. In faba bean, the response to water deficit is highly dynamic and genotype-dependent. Some genotypes follow a ``water-saving'' strategy, closing stomata early and exhibiting a rapid but controlled decline in photosynthetic efficiency, while others follow a ``water-spending'' strategy, maintaining growth until a sudden hydraulic failure occurs. By processing the entire 22-round measurement sequence, the transformer can capture these varied trajectories and distinguish between transient environmental fluctuations (e.g., a particularly hot afternoon) and the cumulative, persistent effects of soil water depletion.

The use of continuous sinusoidal positional encodings was critical for this longitudinal modeling. Unlike standard NLP tasks where tokens are equidistant, phenotyping experiments are often subject to irregular imaging intervals due to logistical constraints or sensor availability. Our encoding method allowed the model to correctly interpret the actual days after germination (DAG), providing it with a sense of the plant's developmental age and the rate of stress progression. This temporal awareness is what enables the model to identify the ``onset'' point with such accuracy. Furthermore, the bidirectional nature of the transformer allowed the model to refine its understanding of the ambiguous pre-symptomatic phase by looking backward from the more obvious symptomatic stages. This retrospectively informed detection is particularly useful for breeding applications where the final tolerance ranking is the primary goal, allowing for a more accurate reconstruction of each genotype's resilience profile.

\subsection{Mechanistic Insights from Feature Analysis: Bridging Visual and Physiological Representations}\label{sec:discussion:features}

The feature analysis presented in the Results section provides critical mechanistic insights into how the framework bridges the physiology-visibility gap. The CLIP embedding space analysis demonstrated that a frozen, general-purpose vision foundation model captures treatment-level visual differences despite not being explicitly trained on drought phenotypes. The partial clustering observed in both t-SNE and UMAP projections (2,905 embeddings from 264 plants) reflects the genotype-dependent nature of visual stress responses, consistent with the model's learned reliance on visual features for late-stage detection. More importantly, the strong correlations between CLIP embeddings and chlorophyll fluorescence parameters (80 of 94 parameters with |r| > 0.3, top three at r $\approx$ 0.85) validate the privileged information paradigm: by training with access to high-resolution fluorescence data, the model learns to associate subtle visual cues with the underlying photosynthetic machinery's state. The temporal dynamics analysis further confirms this mechanistic grounding. The data-driven parameters selected by the model (Fv\_Lss, Fq\_Lss, Fv/Fm\_Lss) exhibited earlier and more pronounced divergence between treatments (DAG 12--15) compared to classic PAM indices (DAG 17--20), aligning precisely with the model's mean onset error of $-1.6$ days. This temporal advantage demonstrates that the transformer's learned attention weights prioritize physiological signals that provide the earliest warning of stress, effectively learning to extract the most informative temporal features from the multimodal data stream. Together, these findings establish that the framework's pre-symptomatic detection capability is not merely a statistical artifact but is grounded in the physiological reality of drought stress progression, where metabolic changes in the photosynthetic apparatus precede visible morphological symptoms.

\subsection{Analysis of Late-timepoint False Positives and the Senescence Confound}\label{sec:discussion:fp}

Despite the overall success, the 6--20\% false positive rate observed in well-watered control plants at the end of the experiment (DAG 31--38) identifies a significant challenge for the field of automated phenotyping. This spike in errors is a direct consequence of the physiological and morphological overlap between drought stress and natural senescence. As faba bean plants age, especially as they transition from vegetative to reproductive stages, the lower leaves naturally undergo programmed cell death. This process involves the degradation of chlorophyll, the remobilization of nitrogen, and a decrease in $F_v/F_m$---shifts that are biochemically and visually remarkably similar to those induced by drought.

Our analysis confirms that the visual and physiological features utilized in this study are not yet specific enough to perfectly decouple these two processes. The model, sensitive to leaf yellowing and declining photosynthetic efficiency, correctly identifies these as signals of decline but incorrectly attributes them to water stress in the control group. This confound is especially problematic for long-term monitoring systems designed to operate throughout the entire life cycle of the crop. Future iterations of this framework should focus on ``un-mixing'' these signals. Potential strategies include the integration of hyperspectral data to identify unique spectral signatures of senescence-specific pigments or the use of multi-task learning where the model is explicitly trained to predict the developmental stage (phenological stage) alongside the stress state. Addressing the senescence confound is essential for ensuring the specificity of early-warning systems, particularly in late-season applications when distinguishing between drought and natural maturity is critical for timing harvest and managing irrigation.

\subsection{Generalization to Other Species and Environmental Scenarios}\label{sec:discussion:generalization}

While this study focused specifically on faba bean, the architecture and underlying philosophy of the temporal multimodal framework are designed for broad applicability across the plant sciences. Leguminous crops, including pea (\textit{Pisum sativum}), lentil (\textit{Lens culinaris}), and soybean (\textit{Glycine max}), share similar physiological drought response mechanisms, such as ABA-mediated stomatal control and the eventual decline in PSII efficiency. Our use of a frozen vision foundation model (CLIP) is a key factor in this potential for generalization. Because CLIP was trained on a massive diversity of image-text pairs, its feature space is not over-specialized to faba bean morphology, suggesting that the same visual representations could be utilized for other crops with minimal adaptation; our ablation results further show that DINOv2 remains a viable alternative backbone.

Furthermore, the adaptive gating mechanism allows the framework to handle the species-specific timing of stress symptoms. For instance, in crops that exhibit more rapid wilting than faba bean, the model would likely learn to transition from physiological to visual features even earlier in the stress cycle. The temporal transformer, with its continuous positional encodings, is also well-suited for different imaging schedules, making it a flexible tool for both high-frequency greenhouse monitoring and lower-frequency field campaigns. Future research should investigate the transferability of the learned temporal priors---specifically the relationship between ChlF decline and visual changes---to other economically important species. Validating the framework under varying soil types and multi-stress scenarios (e.g., combined drought and heat) will be essential for developing truly universal early-warning systems for global agriculture.

\subsection{Implications for Breeding and Precision Agriculture}\label{sec:discussion:implications}

The practical utility of our temporal multimodal framework is two-fold, offering significant benefits to both plant breeders and agronomists. For breeding programs, the ability to accurately rank 44 genotypes based on their physiological stress response represents a major advancement over traditional endpoint measurements such as final biomass or yield. Our LOGO-CV validation results prove that the model's ranking is robust to genotypes it has never seen before, making it a reliable tool for selecting the most resilient accessions from large germplasm panels. The pre-symptomatic detection capability allows breeders to identify ``early responders''---genotypes that prioritize physiological safety---and differentiate them from ``late responders'' that may maintain appearance at the cost of severe internal damage. This high-throughput physiological profiling can significantly accelerate the development of drought-resilient varieties tailored for specific environmental niches.

For precision agriculture, the early-warning signal detected by our model provides a vital lead-time for management intervention. In large-scale greenhouse or field operations, automated irrigation systems could be triggered by the framework's pre-symptomatic stress probability rather than waiting for visible wilting. This ``just-in-time'' irrigation strategy could stabilize yields while drastically reducing water consumption, a critical requirement for sustainable farming in water-scarce regions. Moreover, the framework's reliance on common sensors like RGB cameras (guided by the privileged knowledge of fluorescence) makes it economically viable for deployment. By bridging the gap between high-precision laboratory research and high-throughput industrial applications, this work provides a scalable computational foundation for the future of smart, resilient agriculture.


%% ============================================================
\section{Conclusion}\label{sec:conclusion}
%% ============================================================

This study has successfully developed and validated a temporal multimodal deep learning framework for the pre-symptomatic detection of drought stress in faba bean (\textit{Vicia faba} L.). By synthesizing high-dimensional features from a CLIP vision foundation model with precision chlorophyll fluorescence, environmental data, and vegetation indices, we achieved a robust F1-score of $0.660$ and an AUC of $0.947$ across a genetically diverse panel of 44 genotypes. Our adaptive gating mechanism and temporal transformer encoder proved essential for capturing the longitudinal dynamics of stress and navigating the physiology-visibility gap. Crucially, we provided empirical validation through a three-way triangulation protocol that the framework identifies physiological stress signals significantly before they manifest as visible morphological symptoms. This work demonstrates the power of combining modern AI representations with traditional physiological knowledge, offering a scalable and robust tool for accelerating the breeding of drought-resilient crops and optimizing irrigation management in the face of a changing climate.


%% ============================================================
\section{Data and Code Availability}\label{sec:data}
%% ============================================================

The source code for the temporal multimodal framework, including the adaptive gating module, the temporal transformer implementation, and the evaluation pipelines, is available at \url{https://github.com/chenghao/faba-drought-phenotyping}. The aggregated results summary, pre-symptomatic analysis datasets, and genotype-level metadata are provided in the repository's data folder. Raw imaging data (approximately 11,600 images) and full chlorophyll fluorescence measurement files are available from the corresponding author upon reasonable request.


\section*{Acknowledgements}

This work was supported by the Academy of Finland (Project No. XXXXXXX) and the Strategic Research Council. We thank the staff at the Luke Viikki greenhouse facility for their assistance with plant maintenance and the automated phenotyping platform.

%% CRediT authorship contribution statement
\printcredits

%% Bibliography
\bibliographystyle{cas-model2-names}
\bibliography{references}

\end{document}
